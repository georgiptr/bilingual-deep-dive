{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows WSL 2 Installation\n",
    "```\n",
    "conda create -n dl_project python=3.10\n",
    "conda activate dl_project\n",
    "python -m pip install tensorflow[and-cuda]\n",
    "sudo apt install cmake\n",
    "python -m pip install --upgrade keras-nlp\n",
    "python -m pip install tensorflow_datasets\n",
    "python -m pip install pyyaml h5py\n",
    "```\n",
    "\n",
    "## Guides\n",
    "\n",
    "https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up training and generation, we use preprocessor of length 128\n",
    "# instead of full length 1024.\n",
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=128,\n",
    ")\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beware - uses 14GB of RAM\n",
    "def read_and_filter_training_data():\n",
    "    folder_path = 'bulgarian_books'\n",
    "    data = []\n",
    "\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if len(line) < 2 or line[0] == '>' or line[0] == '@' or line.startswith('D>') or line.startswith('D$') \\\n",
    "                    or line.startswith('E>') or line.startswith('E$') or line.startswith('P>') or line.startswith('P$'):\n",
    "                    continue\n",
    "                line = line.replace('\\r', '').replace('\\t', '')\n",
    "                data.append(line)\n",
    "    return data\n",
    "\n",
    "def develop_fast_load_training_data():\n",
    "    data_file = 'bulgarian_training_data.txt'\n",
    "    with open(data_file, 'r') as file:\n",
    "       data = []\n",
    "       for i in range(10000):\n",
    "           line = next(file)\n",
    "           data.append(line)\n",
    "    return data\n",
    "\n",
    "def load_training_data():\n",
    "    # If the training data is not already preprocessed, it is preprocessed and saved to a file.\n",
    "    # Otherwise, it is just loaded from the file.\n",
    "    data_file = 'bulgarian_training_data.txt'\n",
    "    if not os.path.exists(data_file):\n",
    "        data = read_and_filter_training_data()\n",
    "        with open(data_file, 'w') as file:\n",
    "            for line in data:\n",
    "                file.write(''.join(line))\n",
    "    else:\n",
    "        with open(data_file, 'r') as file:\n",
    "            data = file.readlines()\n",
    "            for i in range(1000000):\n",
    "                line = next(file)\n",
    "                data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data):\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(data)\n",
    "        .batch(256)\n",
    "        #.cache()\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    num_epochs = 6\n",
    "\n",
    "    learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "        5e-4,\n",
    "        decay_steps=train_ds.cardinality() * num_epochs,\n",
    "        end_learning_rate=0.0,\n",
    "    )\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=loss,\n",
    "        weighted_metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    checkpoint_path = \"training/cp-{epoch:02d}.weights.h5\"\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=1)\n",
    "\n",
    "    model.fit(train_ds, epochs=num_epochs, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = load_training_data()\n",
    "# data = develop_fast_load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(gpt2_lm, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(model, text):\n",
    "    return model.generate(text, max_length=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_output(model, text):\n",
    "    print(ask_model(model, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Докато шофираш, за да превключиш скорост, трябва да го сториш следобед. Неговите хора само могат да го убиват — и на\n"
     ]
    }
   ],
   "source": [
    "gpt2_lm.load_weights('training_overnight/cp.ckpt')\n",
    "print_model_output(gpt2_lm, \"Докато шофираш, за да превключиш скорост, трябва да\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
